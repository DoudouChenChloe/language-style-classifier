{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "wound-muslim",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import  pandas  as pd\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "perceived-techno",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read excel\n",
    "data_excel = pd.read_excel('/Users/chloe/Desktop/train_data.xlsx')\n",
    "raw_features = data_excel['sentence']\n",
    "raw_label = data_excel.iloc[:,[1,2,3,4,5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "finite-light",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2671,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "productive-assurance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2671, 5)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "rational-mumbai",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "id_nolabel = []\n",
    "for i in range(len(raw_label)):\n",
    "    if not pd.isnull(raw_label.iloc[i][0]):\n",
    "        labels.append(0)\n",
    "    elif not pd.isnull(raw_label.iloc[i][1]):\n",
    "        labels.append(1)\n",
    "    elif not pd.isnull(raw_label.iloc[i][2]):\n",
    "        labels.append(2)\n",
    "    elif not pd.isnull(raw_label.iloc[i][3]):\n",
    "        labels.append(3)\n",
    "    elif not pd.isnull(raw_label.iloc[i][4]):\n",
    "        labels.append(4)\n",
    "    else :id_nolabel.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "common-desperate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete invalid data\n",
    "train_features=raw_features.drop(id_nolabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "criminal-think",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "class_le = LabelEncoder()\n",
    "labels = class_le.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "british-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "#import self-defined dictionary for e-commerce live streaming\n",
    "jieba.load_userdict('/Users/chloe/Desktop/自有词典.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "corporate-tackle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function defining stopwords \n",
    "def stopwordslist():\n",
    "    stopwords = [line.strip() for line in open('/Users/chloe/Desktop/baidu_stopwords.txt',encoding='UTF-8').readlines()]\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "palestinian-things",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split sentences into words with Jieba \n",
    "def seg_depart(sentence):\n",
    "    # print(\"正在分词\")\n",
    "    sentence_depart = jieba.cut(sentence.strip())\n",
    "    # create a stopwords list \n",
    "    stopwords = stopwordslist()\n",
    "    outstr = ''\n",
    "    # delete the stopwords \n",
    "    for word in sentence_depart:\n",
    "        if word not in stopwords:\n",
    "            if word != '\\t':\n",
    "                outstr += word\n",
    "                outstr += \" \"\n",
    "    return outstr\n",
    "\n",
    "train_features_fenci = [seg_depart(f.replace(\"\\n\", \"\")) for f in train_features]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "outer-might",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorize the words with the tool Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "comprehensive-living",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec  \n",
    "n_dim=60\n",
    "min_count =1\n",
    "w2vmodel=Word2Vec(vector_size=n_dim,min_count=min_count)\n",
    "\n",
    "w2vmodel.build_vocab(train_features_fenci)       \n",
    "w2vmodel.train(train_features_fenci,total_examples=w2vmodel.corpus_count,epochs=10) #train the classifier\n",
    "\n",
    "def m_avgvec(words,w2vmodel):       #vectorize sentences with vectors of each word\n",
    "    return pd.DataFrame([w2vmodel.wv[w]\n",
    "                       for w in words if w in w2vmodel.wv]).agg('mean')\n",
    "\n",
    "train_vec=pd.DataFrame([m_avgvec(a,w2vmodel) for a in train_features_fenci])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "retired-trading",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_vec[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "combined-commercial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "curious-alfred",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dressed-recipient",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save features and labels\n",
    "train_vec.to_csv('/Users/chloe/Desktop/data.csv',index=False)\n",
    "#x = pd.read_csv(open('C:/Users/baikunlong/Desktop/陈豆豆/datasets/CDD_data.csv'))\n",
    "np.savetxt('/Users/chloe/Desktop/Label.csv', labels, delimiter = ',')\n",
    "#y = np.loadtxt(open(\"C:/Users/baikunlong/Desktop/陈豆豆/datasets/CDD_label.csv\",\"rb\"),delimiter=\",\",skiprows=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "portuguese-scratch",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data, split train and test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "metric-image",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "peaceful-impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv(open('/Users/chloe/Desktop/data.csv'))\n",
    "y = np.loadtxt(open('/Users/chloe/Desktop/Label.csv',\"rb\"),delimiter=\",\",skiprows=0)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "straight-looking",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm,metrics\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import roc_curve, auc, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "forced-destruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = np.random.RandomState(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "arctic-briefs",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "medical-engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OneVsRestClassifier(svm.SVC(kernel='linear',probability=True,random_state=random_state,C=1000))\n",
    "clt = model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "sought-alberta",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "emotional-acrylic",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = clt.predict(x_test)\n",
    "label_list = ['Authority','Scarcity','Liking','Emotional contagion','Guarantee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "subject-creativity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall accuracy: 0.768224\n",
      "===========================================\n",
      "acc_for_each_class:\n",
      " [0.75539568 0.76       0.83185841 0.7961165  0.675     ]\n",
      "===========================================\n",
      "average accuracy:0.763674\n",
      "===========================================\n",
      "classification report: \n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          Authority       0.76      0.88      0.81       120\n",
      "           Scarcity       0.76      0.69      0.72       110\n",
      "             Liking       0.83      0.88      0.85       107\n",
      "Emotional contagion       0.80      0.81      0.80       101\n",
      "          Guarantee       0.68      0.56      0.61        97\n",
      "\n",
      "           accuracy                           0.77       535\n",
      "          macro avg       0.76      0.76      0.76       535\n",
      "       weighted avg       0.76      0.77      0.76       535\n",
      "\n",
      "===========================================\n"
     ]
    }
   ],
   "source": [
    "ov_acc = metrics.accuracy_score(y_test_pred,y_test)\n",
    "print(\"overall accuracy: %f\"%(ov_acc))\n",
    "print(\"===========================================\")\n",
    "acc_for_each_class = metrics.precision_score(y_test,y_test_pred,average=None)\n",
    "print(\"acc_for_each_class:\\n\",acc_for_each_class)\n",
    "print(\"===========================================\")\n",
    "avg_acc = np.mean(acc_for_each_class)\n",
    "print(\"average accuracy:%f\"%(avg_acc))\n",
    "print(\"===========================================\")\n",
    "classification_rep = classification_report(y_test,y_test_pred,target_names=label_list)\n",
    "print(\"classification report: \\n\",classification_rep)\n",
    "print(\"===========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-egyptian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-projector",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "substantial-patio",
   "metadata": {},
   "outputs": [],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "simplified-theorem",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read excel\n",
    "data_excel = pd.read_excel('/Users/chloe/Desktop/train_to_predict.xlsx')\n",
    "test_raw_features = data_excel['sentence']\n",
    "name = data_excel['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "vanilla-jesus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6713,)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_raw_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "determined-flour",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split sentences \n",
    "test_features_fenci = [seg_depart(f.replace(\"\\n\", \"\")) for f in test_raw_features]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "burning-receptor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorization \n",
    "from gensim.models.word2vec import Word2Vec  \n",
    "n_dim=60\n",
    "min_count =1\n",
    "w2vmodel=Word2Vec(vector_size=n_dim,min_count=min_count)\n",
    "\n",
    "w2vmodel.build_vocab(test_features_fenci)       \n",
    "w2vmodel.train(test_features_fenci,total_examples=w2vmodel.corpus_count,epochs=10) #training the classifier \n",
    "\n",
    "def m_avgvec(words,w2vmodel):       #vectorizze the sentences\n",
    "    return pd.DataFrame([w2vmodel.wv[w]\n",
    "                       for w in words if w in w2vmodel.wv]).agg('mean')\n",
    "predict_vec=pd.DataFrame([m_avgvec(a,w2vmodel) for a in test_features_fenci])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "understood-cholesterol",
   "metadata": {},
   "outputs": [],
   "source": [
    "#存\n",
    "predict_vec.to_csv('/Users/chloe/Desktop/to_predict_csv.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "double-wrist",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read predict data\n",
    "predict_data = pd.read_csv(open('/Users/chloe/Desktop/to_predict_csv.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "careful-flash",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6713, 60)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "furnished-allah",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = clt.predict(predict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "packed-payroll",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 7)\n",
      "(6713, 7)\n"
     ]
    }
   ],
   "source": [
    "output = pd.DataFrame(columns = ['name' , 'sentence', 'Authority', 'Scarcity', 'Liking', 'Emotional contagion', 'Guarantee'])\n",
    "print(output.shape)\n",
    "for i in range(len(y_test_pred)):\n",
    "    if int(y_test_pred[i])==0:\n",
    "        A1i=1\n",
    "    else: A1i=0\n",
    "        \n",
    "    if int(y_test_pred[i])==1:\n",
    "        A2i=1\n",
    "    else: A2i=0\n",
    "        \n",
    "    if int(y_test_pred[i])==2:\n",
    "        A3i=1\n",
    "    else: A3i=0\n",
    "        \n",
    "    if int(y_test_pred[i])==3:\n",
    "        A4i=1\n",
    "    else: A4i=0\n",
    "        \n",
    "    if int(y_test_pred[i])==4:\n",
    "        A5i=1\n",
    "    else: A5i=0\n",
    "        \n",
    "    tupi = name[i], test_raw_features[i], A1i, A2i, A3i, A4i, A5i\n",
    "    output.loc[i] = tupi\n",
    "    \n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "acoustic-alignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_excel('/Users/chloe/Desktop/to_predict_output.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-collective",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-composer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-boutique",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-monroe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
